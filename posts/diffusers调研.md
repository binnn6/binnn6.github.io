## ğŸ¤—  DğŸ§¨ ffusersè°ƒç ”

> ç”±äºæœ¬äººéç®—æ³•èƒŒæ™¯ã€æœ¬æ–‡æœªæè¿°ä»»ä½•ç®—æ³•åŸç†ã€å°½ç®¡å¦‚æ­¤ï¼Œè¿˜æ˜¯ä¼šæœ‰å¾ˆå¤šåœ°æ–¹è¯´çš„å¯èƒ½ä¸æ­£ç¡®ï¼Œæœ‰ä»»ä½•é—®é¢˜éƒ½æ¬¢è¿æŒ‡æ­£ã€‚

## Stable DiffusionåŸºæœ¬æ¦‚å¿µä¸åŸç†

![sd-pipeline](http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-053448.png)

æ–‡ç”Ÿæ¨ç†é˜¶æ®µä¸»è¦æµç¨‹ï¼š

1. ç”Ÿæˆä¸€ä¸ªéšæœºçš„å™ªå£°ï¼Œå¤–åŠ æ ¹æ®promptç”Ÿæˆprompt_embedsï¼›

2. Stable Diffusionçš„æ ¸å¿ƒé€»è¾‘æ˜¯ä¸€ä¸ªå¾ªç¯ï¼Œæ¯ä¸ªå¾ªç¯å°†é™ä½ä¸€äº›å›¾åƒçš„å™ªå£°ï¼Œå°†å›¾åƒå˜çš„æ›´æ¸…æ™°ã€‚å…¶ä¸­æ¯ä¸ªå¾ªç¯ï¼š

- å°†å›¾åƒAå’Œclip prompt_embedsè¾“å…¥æ¨¡å‹é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œç„¶åå°†å™ªå£°æ®‹å·®ã€å›¾åƒ Aã€step_no è¾“å…¥é‡‡æ ·å™¨é¢„æµ‹æ›´æ¸…æ™°çš„å›¾åƒB;

- å°†å›¾åƒBå’Œclip prompt_embedsè¾“å…¥æ¨¡å‹é¢„æµ‹å™ªå£°æ®‹å·®..å¾ªç¯Næ¬¡,å¾—åˆ°å›¾åƒC

3. æœ€ç»ˆå°†å›¾åƒ C è¾“å…¥ VAE æ¨¡å‹,å¾—åˆ°æ›´åŠ æ¸…æ™°çš„å›¾åƒã€‚

ä»¥ä¸‹æ˜¯å¯¹æ¶‰åŠæ¨¡å—çš„é€šä¿—è§£é‡Šã€‚

### CLIP text-encoder

è¯´ç™½äº†ï¼Œå°±æ˜¯å°†promptè½¬æ¢æˆprompt_embedsï¼Œè€Œè¿™ä¸ªprompt_embedså¯åŒ…å«å›¾åƒç‰¹å¾ã€‚

> CLIP æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€å­¦ä¹ æ¡†æ¶ï¼Œæ¶‰åŠåˆ°åŒæ—¶è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨å’Œå›¾åƒç¼–ç å™¨ã€‚è¿™ç§è”åˆè®­ç»ƒæœ‰åŠ©äºè®©æ–‡æœ¬ç¼–ç å™¨å’Œå›¾åƒç¼–ç å™¨ä¹‹é—´å…±äº«ç›¸åŒçš„è¯­ä¹‰ç©ºé—´ã€‚è¿™ä½¿å¾—å½“æˆ‘ä»¬ä»æ–‡æœ¬ç¼–ç å™¨å¾—åˆ°æ–‡æœ¬çš„å‘é‡è¡¨ç¤ºæ—¶ï¼Œèƒ½å¤Ÿä¸å›¾åƒç¼–ç å™¨ç”Ÿæˆçš„å›¾åƒå‘é‡è¿›è¡Œç›´æ¥æ¯”è¾ƒã€‚
>
> å¾—ç›Šäºè¿™ç§è”åˆè®­ç»ƒï¼ŒCLIP æ–‡æœ¬ç¼–ç å™¨å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤šæ¨¡æ€å­¦ä¹ ä»»åŠ¡ï¼Œæ¯”å¦‚å›¾åƒåˆ†ç±»ã€å›¾åƒæ£€ç´¢ã€å›¾åƒç”Ÿæˆç­‰ã€‚é€šè¿‡åˆ©ç”¨æ–‡æœ¬ç¼–ç å™¨ç”Ÿæˆçš„æ–‡æœ¬å‘é‡è¡¨ç¤ºï¼Œæ¨¡å‹èƒ½æ›´å¥½åœ°å…³è”æ–‡æœ¬å’Œå›¾åƒä¿¡æ¯ï¼Œä»è€Œåœ¨è¿™äº›ä»»åŠ¡ä¸­å®ç°é«˜æ€§èƒ½ã€‚            
>
> -- GPT4ç”Ÿæˆå†…å®¹

> Inspired by [Imagen](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during training and simply uses an CLIP's already trained text encoder, [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).             --- [diffusers_intro colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)

### æ¨¡å‹

å°†prompt_embedsï¼Œå¸¦å™ªå£°çš„å›¾åƒ è¾“å…¥æ¨¡å‹ æ¨ç†å¾—åˆ° å™ªå£°æ®‹å·®ã€‚å³ model(prompt_embeds, å¸¦å™ªå£°çš„å›¾åƒ) =  å™ªå£°æ®‹å·®

>  Without going in too much detail, the model is usually not trained to directly predict a slightly less noisy image, but rather to predict the "noise residual" which is the difference between a less noisy image and the input image (for a diffusion model called "DDPM") or, similarly, the gradient between the two time steps (like the diffusion model called "Score VE"). 

### é‡‡æ ·å™¨

å°†å™ªå£°æ®‹å·®ã€å›¾åƒ ã€æ¨ç†æ­¥éª¤æ•° è¾“å…¥é‡‡æ ·å™¨é¢„æµ‹æ›´æ¸…æ™°çš„å›¾åƒ; å³sample(å™ªå£°æ®‹å·®, æ¨ç†æ­¥éª¤æ•°, å¸¦å™ªå£°çš„å›¾åƒ) = æ›´æ¸…æ™°çš„å›¾åƒ

### VAE

å¯ä»¥ç†è§£ä¸ºä¸€ç§å‹ç¼©ç®—æ³•ã€encode å°±æ˜¯å‹ç¼©ã€å°½å¯èƒ½ä¿ç•™å›¾åƒç‰¹å¾çš„åŒæ—¶å‹ç¼©å›¾ç‰‡çš„å¤§å°ï¼Œè¿™æ ·å¤„ç†æ›´å¿«ã€æ›´èŠ‚çœèµ„æºï¼›decode å°±æ˜¯è§£å‹ç¼©ã€æ¢å¤æˆåŸæœ‰çš„å›¾ç‰‡ï¼› è¿™æ˜¯latent diffusionåˆå¿«åˆé«˜æ•ˆçš„å…³é”®æ‰€åœ¨ã€‚

> The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the U-Net model. The decoder, conversely, transforms the latent representation back into an image.
>
> Since latent diffusion operates on a low dimensional space, it greatly reduces the memory and compute requirements compared to pixel-space diffusion models. For example, the autoencoder used in Stable Diffusion has a reduction factor of 8. This means that an image of shape `(3, 512, 512)` becomes `(3, 64, 64)` in latent space, which requires `8 Ã— 8 = 64` times less memory.

<img src="http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-053444.png" alt="img" style="zoom:67%;" />

## diffusersåº“

å½“å‰aigcæ¦‚å¿µç«çƒ­ï¼Œaiç»˜ç”»ä½œä¸ºå…¶ä¸­çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œéšç€stable diffusionçš„å¼€æºï¼Œå¤§å®¶çº·çº·å¼€å§‹**è‡ªç ”**ã€‚ç›®å‰æ¯”è¾ƒçƒ­é—¨çš„å¼€æºé¡¹ç›®æœ‰ï¼š

[Automatic1111 Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

[ComfyUI](https://github.com/comfyanonymous/ComfyUI)

[InvokeAI](https://github.com/invoke-ai/InvokeAI)

å…¶ä¸­æœ€ä¸ºç«çƒ­çš„æ˜¯[Automatic1111 Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)ï¼ŒåŠŸèƒ½ä¹Ÿæ˜¯æœ€é½å…¨çš„ï¼Œä¸è¿‡ä»£ç ç»“æ„å’Œé£æ ¼å¯¼è‡´å…¶ä¸å¤ªåˆé€‚è‡ªç ”(äºŒæ¬¡å¼€å‘)ã€‚å› æ­¤å°±æƒ³çœ‹æœ‰æ²¡æœ‰æ›´é€‚åˆäºŒæ¬¡å¼€å‘çš„é¡¹ç›®ã€‚

ä¸€ç§æ–¹å¼æ˜¯åƒAutomatic1111 Stable Diffusion WebUIä¸€æ ·ï¼Œç›´æ¥åŸºäº[CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion)ã€[runwayml/stable-diffusion](https://github.com/runwayml/stable-diffusion)ã€[Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion)ã€[https://github.com/crowsonkb/k-diffusion](https://github.com/crowsonkb/k-diffusion)ç­‰å¼€æºåº“è¿›è¡Œå¼€å‘ï¼Œè¿™æ ·å·¥ä½œé‡ä¼šéå¸¸å¤§ã€‚

è€Œå¦å¤–ä¸€ç§æ–¹å¼æ˜¯åŸºäºdiffusersåº“è¿›è¡Œå¼€å‘ã€‚

diffusersæ˜¯huggingfaceå¼€æºçš„ã€ä¸€ä¸ªdiffusion modelsçš„å·¥å…·åº“ï¼ŒåŒ…å«unetã€é‡‡æ ·å™¨ã€vaeç­‰å®ç°ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒå°†ç±»ä¼¼æ–‡ç”Ÿå›¾ã€å›¾ç”Ÿå›¾çš„é€»è¾‘æŠ½è±¡æˆpipelineã€‚

> ğŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. 

 å¦å¤–[InvokeAI](https://github.com/invoke-ai/InvokeAI)çš„åç«¯å·²åˆ‡æ¢ä¸ºæœ‰diffusersé©±åŠ¨ã€‚

ä»¥ä¸‹å†…å®¹ä¸»è¦ä»‹ç»diffusersä½¿ç”¨ã€åŸç†åˆ†æä»¥åŠå¦‚ä½•å¤ç°[Automatic1111 Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)æ‰€æ”¯æŒçš„åŠŸèƒ½ã€‚

### ç®€å•çš„å®ä¾‹

```python
from diffusers import DiffusionPipeline
from torch import autocast
# åŠ è½½æ¨¡å‹é…ç½®ã€åˆå§‹åŒ– pipeline
pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5").to("cuda")
prompt = "a photo of an astronaut riding a horse on mars"

with autocast("cuda"):
    # æ‰§è¡Œæ¨ç†ã€StableDiffusionPipelineçš„__call__å‡½æ•°
    image = pipeline(prompt)[0][0]

image.save("astronaut_rides_horse.png")
```

### æ¦‚å¿µï¼šSchedulers ä¸ Models 

diffusersä¸­çš„ modelsæ˜¯é¢„æµ‹æ®‹å·®ã€è€Œ schedulerså…¶å®å°±æ˜¯ä¸Šæ–‡ä¸­æ‰€è¯´çš„é‡‡æ ·å™¨,ä¸»è¦ä½œç”¨æ˜¯å¢åŠ å™ªå£°ç”¨äºè®­ç»ƒå’Œå°† modelsé¢„æµ‹çš„æ®‹å·®åº”ç”¨åˆ° æœ‰å™ªå£°çš„å›¾åƒä¸Šç”Ÿæˆæ›´æ¸…æ™°çš„å›¾ç‰‡ã€‚(U-Net Model)

> **Schedulers** define the noise schedule which is used to add noise to the model during training, and also define the algorithm to **compute** the slightly less noisy sample given the model output (here `noisy_residual`). 
>
> The model **predict** the noise residual or slightly less noisy image with its trained weights, while the scheduler **computes** the previous sample given the model's output.

Schedulersé…ç½®å‚æ•°

\- `num_train_timesteps` defines the length of the denoising process, e.g. how many timesteps are need to process random gaussian noise to a data sample.

## diffusersä»£ç ç»“æ„

diffusersä»£ç ç»“æ„åˆç†ã€ç®€å•ã€å¤–éƒ¨ä¾èµ–å°‘ï¼Œæ˜¯å¼€å‘äººå‘˜äºŒæ¬¡å¼€å‘diffusion modelsçš„é¦–é€‰ã€‚

```bash
â”œâ”€â”€ docs
â”œâ”€â”€ examples
â”œâ”€â”€ Makefile
â”œâ”€â”€ MANIFEST.in
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts
â”œâ”€â”€ setup.py
â”œâ”€â”€ src
â”‚   â””â”€â”€ diffusers
â”‚       â”œâ”€â”€ commands
â”‚       â”œâ”€â”€ models æ¨¡å‹å®ç°ç›¸å…³ç±»ï¼Œå¦‚ unetã€vae
â”‚       â”‚   â”œâ”€â”€ activations.py
â”‚       â”‚   â”œâ”€â”€ attention_processor.py
â”‚       â”‚   â”œâ”€â”€ attention.py
â”‚       â”‚   â”œâ”€â”€ autoencoder_kl.py
â”‚       â”‚   â”œâ”€â”€ controlnet.py
â”‚       â”‚   â”œâ”€â”€ cross_attention.py
â”‚       â”‚   â”œâ”€â”€ dual_transformer_2d.py
â”‚       â”‚   â”œâ”€â”€ embeddings.py
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ modeling_flax_pytorch_utils.py
â”‚       â”‚   â”œâ”€â”€ modeling_flax_utils.py
â”‚       â”‚   â”œâ”€â”€ modeling_utils.py
â”‚       â”‚   â”œâ”€â”€ prior_transformer.py
â”‚       â”‚   â”œâ”€â”€ resnet.py
â”‚       â”‚   â”œâ”€â”€ t5_film_transformer.py
â”‚       â”‚   â”œâ”€â”€ transformer_2d.py
â”‚       â”‚   â”œâ”€â”€ transformer_temporal.py
â”‚       â”‚   â”œâ”€â”€ unet_1d_blocks.py
â”‚       â”‚   â”œâ”€â”€ unet_1d.py
â”‚       â”‚   â”œâ”€â”€ unet_2d_blocks_flax.py
â”‚       â”‚   â”œâ”€â”€ unet_2d_blocks.py
â”‚       â”‚   â”œâ”€â”€ unet_2d_condition_flax.py
â”‚       â”‚   â”œâ”€â”€ unet_2d_condition.py
â”‚       â”‚   â”œâ”€â”€ unet_2d.py
â”‚       â”‚   â”œâ”€â”€ unet_3d_blocks.py
â”‚       â”‚   â”œâ”€â”€ unet_3d_condition.py
â”‚       â”‚   â”œâ”€â”€ vae.py
â”‚       â”‚   â””â”€â”€ vq_model.py
â”‚       â”œâ”€â”€ pipelines ä»»åŠ¡é€»è¾‘æŠ½è±¡å±‚
â”‚       â”‚   â”œâ”€â”€ audio_diffusion
â”‚       â”‚   â”œâ”€â”€ controlnet
â”‚       â”‚   â”œâ”€â”€ dance_diffusion
â”‚       â”‚   â”œâ”€â”€ ddim
â”‚       â”‚   â”œâ”€â”€ ddpm
â”‚       â”‚   â”œâ”€â”€ kandinsky
â”‚       â”‚   â”œâ”€â”€ latent_diffusion
â”‚       â”‚   â”œâ”€â”€ latent_diffusion_uncond
â”‚       â”‚   â”œâ”€â”€ repaint
â”‚       â”‚   â”œâ”€â”€ score_sde_ve
â”‚       â”‚   â”œâ”€â”€ semantic_stable_diffusion
â”‚       â”‚   â”œâ”€â”€ spectrogram_diffusion
â”‚       â”‚   â”œâ”€â”€ stable_diffusion
â”‚       â”‚   â”œâ”€â”€ stable_diffusion_safe
â”‚       â”‚   â”œâ”€â”€ text_to_video_synthesis
â”‚       â”‚   â”œâ”€â”€ unclip
â”‚       â”‚   â”œâ”€â”€ unidiffuser
â”‚       â”‚   â”œâ”€â”€ versatile_diffusion
â”‚       â”‚   â””â”€â”€ vq_diffusion
â”‚       â”œâ”€â”€ schedulers  å¯¹åº”sd webuiçš„é‡‡æ ·å™¨
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ README.md
â”‚       â”‚   â”œâ”€â”€ scheduling_ddim_flax.py
â”‚       â”‚   â”œâ”€â”€ scheduling_ddim.py
â”‚       â”‚   â”œâ”€â”€ scheduling_ddpm.py
â”‚       â”‚   â”œâ”€â”€ scheduling_dpmsolver_sde.py
â”‚       â”‚   â”œâ”€â”€ scheduling_euler_ancestral_discrete.py
â”‚       â”‚   â”œâ”€â”€ scheduling_euler_discrete.py
â”‚       â”‚   â”œâ”€â”€ scheduling_heun_discrete.py
â”‚       â”‚   â”œâ”€â”€ scheduling_ipndm.py
â”‚       â”‚   â”œâ”€â”€ scheduling_karras_ve.py
â”‚       â”‚   â”œâ”€â”€ scheduling_k_dpm_2_ancestral_discrete.py
â”‚       â”‚   â”œâ”€â”€ scheduling_k_dpm_2_discrete.py
â”‚       â”‚   â”œâ”€â”€ scheduling_lms_discrete.py
â”‚       â”‚   â”œâ”€â”€ scheduling_pndm.py
â”‚       â”‚   â”œâ”€â”€ scheduling_repaint.py
â”‚       â”‚   â”œâ”€â”€ scheduling_sde_ve.py
â”‚       â”‚   â”œâ”€â”€ scheduling_sde_vp.py
â”‚       â”‚   â”œâ”€â”€ scheduling_unclip.py
â”‚       â”‚   â”œâ”€â”€ scheduling_utils.py
â”‚       â”‚   â””â”€â”€ scheduling_vq_diffusion.py
â”‚       â””â”€â”€ utils
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ models
â”‚   â”œâ”€â”€ pipelines
â”‚   â””â”€â”€ schedulers
â”œâ”€â”€ _typos.toml
â””â”€â”€ utils
```

## diffusersæ ¸å¿ƒé€»è¾‘

###Stable DiffusionåŠ è½½æ¨¡å‹ã€åˆå§‹åŒ– pipeline

#### from_pretrained

ä¸‹è½½æˆ–è€…ä½¿ç”¨æœ¬åœ°çš„æ¨¡å‹ã€æ ¹æ®model_index.jsonåˆå§‹åŒ– pipeline ä»¥åŠå…¶æ‰€éœ€è¦çš„æ¨¡å—

1. ä¸‹è½½æ¨¡å‹æˆ–è€…ç›´æ¥ä½¿ç”¨æœ¬åœ°çš„æ¨¡å‹ï¼›pretrained_model_name_or_path 

   ```bash
   models--runwayml--stable-diffusion-v1-5
   â”œâ”€â”€ blobs
   â”‚   â”œâ”€â”€ 193490b58ef62739077262e833bf091c66c29488058681ac25cf7df3d8190974
   â”‚   â”œâ”€â”€ 1a02ee8abc93e840ffbcb2d68b66ccbcb74b3ab3
   â”‚   â”œâ”€â”€ 1b134cded8eb78b184aefb8805b6b572f36fa77b255c483665dda93
   â”‚   â”œâ”€â”€ 2c2130b544c0c5a72d5d00da071ba130a9800fb2
   â”‚   â”œâ”€â”€ 469be27c5c010538f845f518c4f5e8574c78f7c8
   â”‚   â”œâ”€â”€ 4d3e873ab5086ad989f407abd50fdce66db8d657
   â”‚   â”œâ”€â”€ 5294955ff7801083f720b34b55d0f1f51313c5c5
   â”‚   â”œâ”€â”€ 55d78924fee13e4220f24320127c5f16284e13b9
   â”‚   â”œâ”€â”€ 5ba7bf706515bc60487ad0e1816b4929b82542d6
   â”‚   â”œâ”€â”€ 5dbd88952e7e521aa665e5052e6db7def3641d03
   â”‚   â”œâ”€â”€ 76e821f1b6f0a9709293c3b6b51ed90980b3166b
   â”‚   â”œâ”€â”€ 770a47a9ffdcfda0b05506a7888ed714d06131d60267e6cf52765d61cf59fd67
   â”‚   â”œâ”€â”€ 82d05b0e688d7ea94675678646c427907419346e
   â”‚   â”œâ”€â”€ c7da0e21ba7ea50637bee26e81c220844defdf01aafca02b2c42ecd
   â”‚   â””â”€â”€ daf7e2e2dfc64fb437a2b44525667111b00cb9fc
   â”œâ”€â”€ refs
   â”‚   â””â”€â”€ main
   â””â”€â”€ snapshots
       â””â”€â”€ aa9ba505e1973ae5cd05f5aedd345178f52f8e6a
           â”œâ”€â”€ feature_extractor
           â”‚   â””â”€â”€ preprocessor_config.json -> ../../../blobs/5294955ff7801083f720b34b55d0f1f51313c5c5
           â”œâ”€â”€ model_index.json -> ../../blobs/daf7e2e2dfc64fb437a2b44525667111b00cb9fc
           â”œâ”€â”€ safety_checker
           â”‚   â”œâ”€â”€ config.json -> ../../../blobs/5dbd88952e7e521aa665e5052e6db7def3641d03
           â”‚   â””â”€â”€ pytorch_model.bin -> ../../../blobs/193490b58ef62739077262e833bf091c66c29488058681ac25cf7df3d8190974
           â”œâ”€â”€ scheduler
           â”‚   â””â”€â”€ scheduler_config.json -> ../../../blobs/82d05b0e688d7ea94675678646c427907419346e
           â”œâ”€â”€ text_encoder
           â”‚   â”œâ”€â”€ config.json -> ../../../blobs/4d3e873ab5086ad989f407abd50fdce66db8d657
           â”‚   â””â”€â”€ pytorch_model.bin -> ../../../blobs/770a47a9ffdcfda0b05506a7888ed714d06131d60267e6cf52765d61cf59fd67
           â”œâ”€â”€ tokenizer
           â”‚   â”œâ”€â”€ merges.txt -> ../../../blobs/76e821f1b6f0a9709293c3b6b51ed90980b3166b
           â”‚   â”œâ”€â”€ special_tokens_map.json -> ../../../blobs/2c2130b544c0c5a72d5d00da071ba130a9800fb2
           â”‚   â”œâ”€â”€ tokenizer_config.json -> ../../../blobs/5ba7bf706515bc60487ad0e1816b4929b82542d6
           â”‚   â””â”€â”€ vocab.json -> ../../../blobs/469be27c5c010538f845f518c4f5e8574c78f7c8
           â”œâ”€â”€ unet
           â”‚   â”œâ”€â”€ config.json -> ../../../blobs/1a02ee8abc93e840ffbcb2d68b66ccbcb74b3ab3
           â”‚   â””â”€â”€ diffusion_pytorch_model.bin -> ../../../blobs/c7da0e21ba7ea50637bee26e81c220844defdf01aafca02b2c42ecd
           â””â”€â”€ vae
               â”œâ”€â”€ config.json -> ../../../blobs/55d78924fee13e4220f24320127c5f16284e13b9
               â””â”€â”€ diffusion_pytorch_model.bin -> ../../../blobs/1b134cded8eb78b184aefb8805b6b572f36fa77b255c483665dda93
   ```

2. åŠ è½½æ¨¡å‹ä¸‹ç›®å½•ä¸‹çš„`model_index.json`åˆ°`config_dict`;

   å…¶å®ä»è¿™é‡Œå°±å¯ä»¥çœ‹å‡ºæ¥ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œschedulerã€unetã€text_encoderã€tokenizerã€vaeéƒ½å·²ç»ç¡®å®šäº†ã€‚ä½†å®é™…æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬å…¶å®å¯ä»¥æ›´æ¢schedulerï¼ˆé‡‡æ ·å™¨ï¼‰ã€‚

   ```json
   {
     "_class_name": "StableDiffusionPipeline",
     "_diffusers_version": "0.6.0",
     "feature_extractor": [
       "transformers",
       "CLIPImageProcessor"
     ],
     "safety_checker": [
       "stable_diffusion",
       "StableDiffusionSafetyChecker"
     ],
     "scheduler": [
       "diffusers",
       "PNDMScheduler"
     ],
     "text_encoder": [
       "transformers",
       "CLIPTextModel"
     ],
     "tokenizer": [
       "transformers",
       "CLIPTokenizer"
     ],
     "unet": [
       "diffusers",
       "UNet2DConditionModel"
     ],
     "vae": [
       "diffusers",
       "AutoencoderKL"
     ]
   }
   ```


3. æ ¹æ®`config_dict["_class_name"]`åŠ è½½å¯¹åº”çš„pipeline class;
4. æ ¹æ®`model_index.json`è®¡ç®—åˆå§‹åŒ–ç®¡çº¿éœ€è¦çš„moduleï¼›  init_dict, unused_kwargs, _ = pipeline_class.extract_init_dict(config_dict, **kwargs)
5. åŠ è½½ pipeline éœ€è¦çš„ model, ä½œä¸ºåˆå§‹åŒ– pipeline çš„å‚æ•°init_kwargsï¼›
6. åˆå§‹åŒ– pipelineï¼Œå³è°ƒç”¨`StableDiffusionPipeline.__init`__
#### from_cpkt

æ”¯æŒåŠ è½½safetensorsæ–‡ä»¶

### æ‰§è¡Œæ¨ç†

```python
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 50,
        guidance_scale: float = 7.5,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_images_per_prompt: Optional[int] = 1,
        eta: float = 0.0,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.FloatTensor] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        output_type: Optional[str] = "pil",
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
    )
```

1. å‚æ•°æ ¡éªŒã€åˆå§‹åŒ– heightã€widthã€batch_sizeã€prompt_embedsã€timestepsã€latents

2. å¾ªç¯æ¨ç†\é™å™ª

   - å°†å›¾åƒå™ªå£°è¾“å…¥ unetã€unet é¢„æµ‹å™ªå£°æ®‹å·®

   - å°†å›¾åƒå™ªå£°ã€å™ªå£°æ®‹å·®è¾“å…¥Scheduler(å³é‡‡æ ·å™¨)ç”Ÿæˆæ–°çš„å›¾åƒå™ªå£°
   - å¾ªç¯ç¬¬ä¸€æ­¥ã€å°†æ–°çš„å›¾åƒå™ªå£°è¾“å…¥ unetï¼› ä¸€ç›´å¾ªç¯num_inference_stepsæ¬¡

   ```python
           # 7. Denoising loop
           num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
           with self.progress_bar(total=num_inference_steps) as progress_bar:
               for i, t in enumerate(timesteps):
                   # expand the latents if we are doing classifier free guidance
                   latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
                   latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)
   
                   # predict the noise residual
                   noise_pred = self.unet(
                       latent_model_input,
                       t,
                       encoder_hidden_states=prompt_embeds,
                       cross_attention_kwargs=cross_attention_kwargs,
                   ).sample
   
                   # perform guidance
                   if do_classifier_free_guidance:
                       noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
                       noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
   
                   # compute the previous noisy sample x_t -> x_t-1
                   latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample
   
                   # call the callback, if provided
                   if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):
                       progress_bar.update()
                       if callback is not None and i % callback_steps == 0:
                           callback(i, t, latents)
   ```

3. vae decodeã€safety checker (å®‰å…¨æ£€æµ‹ã€æ¯”å¦‚é‰´é»„)

## å¦‚ä½•ç”Ÿæˆä¸[stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)ç›¸åŒçš„ç»“æœ
ä»¥ä¸‹caseä¸­ä½¿ç”¨çš„æ¨¡å‹ä¸å‚æ•°

- SD Model: [majicmix-realistic](https://civitai.com/models/43331/majicmix-realistic) v6

- LoRA Model: [MoXin](https://civitai.com/models/12597) v1

- é‡‡æ ·å™¨ï¼šEuler A

### åŸºæœ¬åŠŸèƒ½

é’ˆå¯¹äºç®€å•çš„æç¤ºè¯ï¼Œå’Œsd webuiç”Ÿæˆçš„å›¾æ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚

```python
import torch
from torch import autocast
from diffusers import EulerAncestralDiscreteScheduler,StableDiffusionPipeline
from transformers import CLIPImageProcessor, CLIPProcessor, CLIPTextModel,CLIPTokenizer
from compel import Compel
from diffusers.utils import get_class_from_dynamic_module

pipeline = StableDiffusionPipeline.from_ckpt(
    "/data/stable-diffusion-webui/models/Stable-diffusion/majicmixRealistic_v6.safetensors"
).to("cuda")
pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)
pipeline.safety_checker = None // é»˜è®¤ä¼šå¯ç”¨safety_checker
prompt0 = "1girl"

generator = torch.Generator(device="cuda").manual_seed(1432216924)


with autocast("cuda"):
    images = pipeline(
        prompt0,
        width = 512,
        height = 512,
        num_images_per_prompt = 1,
        generator=generator,
        num_inference_steps=30,
        guidance_scale=7).images
    

images[0]
```

<img src="http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-053520.png" alt="output" style="zoom: 67%;" />

### å¤æ‚ä¸€äº›çš„æç¤ºè¯

æµ‹è¯•ä¸­ä¸»è¦å‘ç°ä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜ä¼šå¯¼è‡´å‡ºå›¾ä¸ä¸€è‡´ï¼š

- diffuserså’Œ[stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)æç¤ºè¯æƒé‡è¯­æ³•ä¸ä¸€è‡´; 

  [diffusers weighted_prompts](https://huggingface.co/docs/diffusers/main/en/using-diffusers/weighted_prompts) 

  [sd webui attention emphasis](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#attentionemphasis)

- diffusersæœ‰æœ€å¤§ 77æç¤ºè¯tokençš„é™åˆ¶ï¼›

  > CLIP (Contrastive Language-Image Pretraining) æ¨¡å‹çš„è¾“å…¥æœ‰ 75 ä¸ª token çš„é™åˆ¶ï¼Œä¸»è¦æ˜¯ç”±äºåœ¨è®­ç»ƒ CLIP æ¨¡å‹æ—¶ï¼Œè®¾è®¡è€…é€‰æ‹©äº†ä¸€ä¸ªå›ºå®šçš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚è¿™ä¸ªé™åˆ¶æœ‰åŠ©äºåœ¨è®¡ç®—æ–¹é¢ä¿æŒæ•ˆç‡ï¼Œå¹¶é™åˆ¶æ¨¡å‹å¤§å°å’Œå†…å­˜ä½¿ç”¨ã€‚
  >
  > åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼Œè¾“å…¥æ•°æ®é€šå¸¸å…·æœ‰ä¸åŒçš„é•¿åº¦å’Œå˜åŒ–ã€‚ä¸ºäº†ç®€åŒ–è®¡ç®—å’Œæé«˜æ•ˆç‡ï¼Œç ”ç©¶äººå‘˜é€šå¸¸ä¼šè®¾å®šä¸€ä¸ªæœ€å¤§åºåˆ—é•¿åº¦ï¼Œä½¿å¾—æ¨¡å‹åœ¨å†…å­˜å’Œè®¡ç®—æœºèƒ½åŠ›èŒƒå›´å†…é«˜æ•ˆè¿è¡Œã€‚å¯¹äº CLIP æ¨¡å‹ï¼Œç ”ç©¶è€…è®¾å®šäº† 75 ä¸ª token çš„æœ€å¤§é•¿åº¦ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨ CLIP å¤„ç†æ–‡æœ¬æ•°æ®æ—¶ï¼Œéœ€è¦å°†è¾“å…¥æˆªæ–­æˆ–å¡«å……ä¸º 75 ä¸ª tokenã€‚
  >
  > è¿™ä¸ªé™åˆ¶å¹¶ä¸æ„å‘³ç€ CLIP æ¨¡å‹åªèƒ½å¤„ç†ç”± 75 ä¸ª token æ„æˆçš„æ•°æ®ã€‚åœ¨å®è·µä¸­ï¼Œæ‚¨å¯ä»¥å°†è¾ƒé•¿çš„è¾“å…¥åˆ†å‰²æˆå¤šä¸ªçŸ­æ®µï¼Œå¹¶åˆ†åˆ«å¤„ç†ï¼Œä½†è¿™éœ€è¦é¢å¤–çš„é¢„å¤„ç†æ­¥éª¤ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦ä¸ºè¶…å‡ºé™åˆ¶çš„è¾“å…¥éƒ¨åˆ†é€‰æ‹©æ›´åˆé€‚çš„æˆªæ–­ç­–ç•¥ï¼ˆå¦‚ç¡®ä¿ä¿ç•™è¯­æ³•ç»“æ„å’Œä¸»è¦æ„ä¹‰ï¼‰ã€‚
  >
  > æ€»ä¹‹ï¼ŒCLIP æ¨¡å‹å…·æœ‰ 75 ä¸ª token çš„é™åˆ¶æ˜¯ä¸ºäº†åœ¨è®­ç»ƒå’Œåº”ç”¨é˜¶æ®µä¿æŒå†…å­˜å’Œè®¡ç®—æ•ˆç‡ã€‚è¿™ç§é™åˆ¶åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æŸäº›é¢å¤–çš„é¢„å¤„ç†å’Œé€‚åº”æ­¥éª¤ã€‚ç„¶è€Œï¼Œé€šè¿‡é€‰æ‹©åˆé€‚çš„æˆªæ–­æˆ–å¡«å……ç­–ç•¥ï¼ŒCLIP æ¨¡å‹ä»ç„¶å¯ä»¥å¹¿æ³›åº”ç”¨äºå„ç§è‡ªç„¶è¯­è¨€å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚     --- GPT4ç”Ÿæˆå†…å®¹

ç¤¾åŒºç‰ˆæœ¬çš„Stable Diffusionå®ç°äº†ä¸sd webuiä¸€æ ·çš„åŠŸèƒ½ã€‚[Long Prompt Weighting Stable Diffusion community pipeline](https://github.com/huggingface/diffusers/blob/main/examples/community/lpw_stable_diffusion.py)

è‡³äºä¸ºä»€ä¹ˆä¸ç›´æ¥åŠ åˆ°StableDiffusionPipelineï¼Œæˆ‘ç†è§£åŸå› æœ‰ä¸¤ä¸ªï¼š

1. diffusersåˆ›å»ºpipelineå¾ˆç®€å•ï¼Œé¼“åŠ±ç”¨æˆ·åˆ›å»ºè‡ªå·±çš„pipelineï¼›

2. æç¤ºè¯æƒé‡ã€tokenæ•°é™åˆ¶édiffusersçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸”æ²¡å¿…è¦å’Œsd webuiä¿æŒä¸€è‡´ã€‚


ä»¥ä¸‹ä»£ç ä¸ºä½¿ç”¨diffusers [Long Prompt Weighting Stable Diffusion community pipeline](https://github.com/huggingface/diffusers/blob/main/examples/community/lpw_stable_diffusion.py)ç¤ºä¾‹,ç»“æœä¸sd webuiå®Œå…¨ä¸€è‡´ï¼š

```python
import torch
from torch import autocast
from diffusers import EulerAncestralDiscreteScheduler,StableDiffusionPipeline
from transformers import CLIPImageProcessor, CLIPProcessor, CLIPTextModel,CLIPTokenizer
from diffusers.utils import get_class_from_dynamic_module


stablepipeline = StableDiffusionPipeline.from_ckpt(
    "/data/stable-diffusion-webui/models/Stable-diffusion/majicmixRealistic_v6.safetensors",
).to("cuda")

LPWStableDiffusionPipeline = get_class_from_dynamic_module("lpw_stable_diffusion", module_file="lpw_stable_diffusion.py")
pipeline = LPWStableDiffusionPipeline(**stablepipeline.components)
pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)
pipeline.safety_checker = None

pipeline.to("cuda")
prompt = "((wallpaper 8k)), ((high detailed)), ((masterpiece)), ((best quality:1.2)), ((hdr)), ((absurdres)), ((RAW photo)), 1girl, green eyes, medium hair, bangs, portrait, upper body, shoulder, shirt, simple background, blue background"
negative_prompt = "easynegative,ng_deepnegative_v1_75t, badhandv4,(worst quality:2),(low quality:2),(normal quality:2),lowres,bad anatomy,bad hands,normal quality,((monochrome)),((grayscale)),((watermark)),"

generator = torch.Generator(device="cuda").manual_seed(1432216924)

with autocast("cuda"):
    images = pipeline(
        prompt,
        width = 512,
        height = 512,
        num_images_per_prompt = 1,
        generator=generator,
        negative_prompt = negative_prompt,
        num_inference_steps=30,
        guidance_scale=7).images
images[0]
```

<img src="http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-053525.png" alt="00006-1432216924" style="zoom: 67%;" />

### LoRAæ”¯æŒ

#### å•LoRAæ”¯æŒ

ç›®å‰å·²å‘å¸ƒç‰ˆæœ¬(diffusers [v0.18.1](https://github.com/huggingface/diffusers/releases/tag/v0.18.1))ä»…æ”¯æŒattention layersï¼Œ æ•ˆæœä¸å¤ªå¥½ã€‚å·²ç»æœ‰äººæäº¤pr [Support to load Kohya-ss style LoRA file format (without restrictions)](https://github.com/huggingface/diffusers/pull/3756),å°šæœªåˆå¹¶ã€‚

å½“ç„¶é™¤æ­¤ä¹‹å¤–è¿˜æœ‰å…¶ä»–ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚[Unload lora weights after they have been loaded](https://github.com/huggingface/diffusers/issues/4027)

> Currently, LoRA is only supported for the attention layers of the `UNet2DConditionalModel`. We also support fine-tuning the text encoder for DreamBooth with LoRA in a limited capacity. Fine-tuning the text encoder for DreamBooth generally yields better results, but it can increase compute usage.    [Low-Rank Adaptation of Large Language Models (LoRA)](https://huggingface.co/docs/diffusers/training/lora)

>Usually, LoRA fine-tuning of the text encoder along with the UNet leads to better results than LoRA fine-tuning the UNet alone. A reference PR that might be relevant here: [#3180](https://github.com/huggingface/diffusers/pull/3180).    [Docs for LoRA are confusing](https://github.com/huggingface/diffusers/issues/3219)

> Discussed in [#3064 (comment)](https://github.com/huggingface/diffusers/issues/3064#issuecomment-1544159510), PR [#3437](https://github.com/huggingface/diffusers/pull/3437). In the PR [#3437](https://github.com/huggingface/diffusers/pull/3437), we implemented the loading of Kohya-ss style LoRA, but this was only support for the Attention part, which had already been implemented in Diffusers. Kohya-ss style LoRA is not only extended to Attention, but also to `ClipMLP`, `Transformer2DModel`'s `proj_in`, `proj_out`, and so on. This PR will support these remaining issues.
>
> [Support to load Kohya-ss style LoRA file format (without restrictions)](https://github.com/huggingface/diffusers/pull/3756)

> [failed to use the feature of supporting for A1111 LoRA](https://github.com/huggingface/diffusers/issues/3725)

```python
import torch
from torch import autocast
from diffusers import EulerAncestralDiscreteScheduler,StableDiffusionPipeline
from transformers import CLIPImageProcessor, CLIPProcessor, CLIPTextModel,CLIPTokenizer
from compel import Compel
from diffusers.utils import get_class_from_dynamic_module


stablepipeline = StableDiffusionPipeline.from_ckpt(
    "/data/stable-diffusion-webui/models/Stable-diffusion/majicmixRealistic_v6.safetensors"
).to("cuda")
LPWStableDiffusionPipeline = get_class_from_dynamic_module("lpw_stable_diffusion", module_file="lpw_stable_diffusion.py")
pipeline = LPWStableDiffusionPipeline(**stablepipeline.components)

pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)
pipeline.feature_extractor = CLIPImageProcessor.from_dict(pipeline.feature_extractor.to_dict())
pipeline.safety_checker = None
pipeline.load_lora_weights("/data/stable-diffusion-webui/models/Lora", weight_name="moxinv1.khzz.safetensors")

prompt = "1girl"
generator = torch.Generator(device="cuda").manual_seed(1432216924)


with autocast("cuda"):
    images = pipeline(
        prompt0,
        width = 512,
        height = 512,
        num_images_per_prompt = 1,
        generator=generator,
        num_inference_steps=30,
        cross_attention_kwargs={"scale": 1},
        guidance_scale=7).images
    

images[0]
```

<center>
<figure>
<img src="http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-030013.png" alt="before-face-store-03" style="zoom:67%;" />
<img src="http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-025959.png" alt="00018-1432216924" style="zoom:67%;" />
</figure>
</center>
<center>å·¦å›¾ä¸ºdiffusers[v0.18.1]çš„æ•ˆæœï¼Œå³å›¾ä¸ºåº”ç”¨prä¹‹åçš„æ•ˆæœ(ä¸sd webuiå®Œå…¨ä¸€è‡´)</center>

#### å¤šLoRAæ”¯æŒ

å°šæœªæ”¯æŒã€[Support for adding multiple LoRA layers to Diffusers](https://github.com/huggingface/diffusers/issues/2613#top)

### é¢éƒ¨ä¿®å¤ - CodeFormer

[CodeFormer repo](https://github.com/sczhou/CodeFormer)

```python
# Clone CodeFormer and enter the CodeFormer folder
!git clone https://github.com/sczhou/CodeFormer.git
%cd  CodeFormer

# Set up the environment
# Install python dependencies
!pip install -r requirements.txt
# Install basicsr
!python basicsr/setup.py develop

# Download the pre-trained model
!python scripts/download_pretrained_models.py facelib
!python scripts/download_pretrained_models.py CodeFormer
```

å°†éœ€è¦ä¿®å¤çš„å›¾åƒæ”¾å…¥inputsç›®å½•ï¼Œæ‰§è¡Œå‘½ä»¤åå¾—åˆ°ä¿®å¤åçš„å›¾ç‰‡

```python
# Inference the uploaded images
#@markdown `CODEFORMER_FIDELITY`: Balance the quality (lower number) and fidelity (higher number)<br>
# you can add '--bg_upsampler realesrgan' to enhance the background
CODEFORMER_FIDELITY = 0.7 #@param {type:"slider", min:0, max:1, step:0.01}
#@markdown `BACKGROUND_ENHANCE`: Enhance background image with Real-ESRGAN<br>
BACKGROUND_ENHANCE = True #@param {type:"boolean"}
#@markdown `FACE_UPSAMPLE`: Upsample restored faces for high-resolution AI-created images<br>
FACE_UPSAMPLE = False #@param {type:"boolean"}
if BACKGROUND_ENHANCE:
  if FACE_UPSAMPLE:
    !python inference_codeformer.py -w $CODEFORMER_FIDELITY --input_path inputs --bg_upsampler realesrgan --face_upsample
  else:
    !python inference_codeformer.py -w $CODEFORMER_FIDELITY --input_path inputs --bg_upsampler realesrgan
else:
  !python inference_codeformer.py -w $CODEFORMER_FIDELITY --input_path inputs
```


### Hires.fix
diffusersä¸­çš„é«˜æ¸…ä¿®å¤å¹¶ä¸åƒsd webuié‚£æ ·å”¾æ‰‹å¯å¾—ï¼Œéœ€è¦æˆ‘ä»¬äº†è§£æ¯ä¸ªæ­¥éª¤çš„åŸç†ï¼Œç„¶åç»„åˆä½¿ç”¨ã€‚

[does diffusers have the equivalent to hires fix from A1111?](https://github.com/huggingface/diffusers/issues/3429)

[Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN)

#### sd webuiæ˜¯å¦‚ä½•å®ç°Hires.fixçš„?

txt2imgçš„æ ¸å¿ƒé€»è¾‘ä½äº

- æ–‡ä»¶: processing.py

- ç±»:  `StableDiffusionProcessingTxt2Img(StableDiffusionProcessing)` 

- å‡½æ•°: `sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts`

![image-20230712152457898](http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-054317.png)

å¦‚ä»£ç æ‰€ç¤ºï¼Œå…¶å®text2img + Hires.fixå…¶å®å°±æ˜¯text2img + upscaler + img2imgã€‚

å…¶ä¸­ç‰¹åˆ«æ³¨æ„Upscalerä¸ºlatentæ—¶ï¼Œæ˜¯å°†å›¾åƒé€šè¿‡`torch.nn.functional.interpolate`å¤„ç†è·å¾—samplesï¼›

> `torch.nn.functional.interpolate` æ˜¯ä¸€ä¸ª PyTorch å‡½æ•°ï¼Œç”¨äºè°ƒæ•´ï¼ˆæ”¾å¤§æˆ–ç¼©å°ï¼‰å¼ é‡ï¼ˆé€šå¸¸æ˜¯å›¾åƒæˆ–ç‰¹å¾æ˜ å°„ï¼‰çš„å°ºå¯¸ã€‚è¿™ä¸ªå‡½æ•°åœ¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­å¾ˆå¸¸è§ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦è°ƒæ•´å›¾åƒæˆ–ç‰¹å¾æ˜ å°„å¤§å°ä»¥é€‚åº”æ¨¡å‹è¾“å…¥è¦æ±‚ã€è¿›è¡Œä¸Šé‡‡æ ·æˆ–ä¸‹é‡‡æ ·æ“ä½œæ—¶ã€‚   
>
> ---GPT4ç”Ÿæˆå†…å®¹

è€Œå…¶ä»–æƒ…å†µåˆ™æ˜¯è°ƒç”¨å¯¹åº”çš„æ¨¡å‹é¢„æµ‹å¾—åˆ°å¯¹åº”å›¾åƒï¼Œç„¶åvae encodeè·å¾—samplesã€‚Upscaleré˜¶æ®µäº§ç”Ÿçš„samplesä¸ºimg2imgé˜¶æ®µçš„è¾“å…¥ï¼›è¯¦ç»†ä»£ç å¦‚ä¸‹ï¼š

![image-20230712210733311](http://devops-1255386119.cos.ap-beijing.myqcloud.com/2023-07-13-054302.png)

å…¶ä¸­upscalerç”¨åˆ°äº†[Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN),æ¨¡å‹åˆ—è¡¨å¦‚ä¸‹ï¼š

```python
def get_realesrgan_models(scaler):
    try:
        from basicsr.archs.rrdbnet_arch import RRDBNet
        from realesrgan.archs.srvgg_arch import SRVGGNetCompact
        models = [
            UpscalerData(
                name="R-ESRGAN General 4xV3",
                path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth",
                scale=4,
                upscaler=scaler,
                model=lambda: SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')
            ),
            UpscalerData(
                name="R-ESRGAN General WDN 4xV3",
                path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth",
                scale=4,
                upscaler=scaler,
                model=lambda: SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')
            ),
            UpscalerData(
                name="R-ESRGAN AnimeVideo",
                path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth",
                scale=4,
                upscaler=scaler,
                model=lambda: SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')
            ),
            UpscalerData(
                name="R-ESRGAN 4x+",
                path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth",
                scale=4,
                upscaler=scaler,
                model=lambda: RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)
            ),
            UpscalerData(
                name="R-ESRGAN 4x+ Anime6B",
                path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth",
                scale=4,
                upscaler=scaler,
                model=lambda: RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)
            ),
            UpscalerData(
                name="R-ESRGAN 2x+",
                path="https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth",
                scale=2,
                upscaler=scaler,
                model=lambda: RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)
            ),
        ]
        return models
    except Exception:
        print("Error making Real-ESRGAN models list:", file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
```

#### diffusersä¸­åº”è¯¥å¦‚ä½•å®ç°

å…¶å®å¾ˆç®€å•ï¼Œè·Ÿsd webuiä¸€æ ·ï¼ŒæŠŠæ–‡ç”Ÿå›¾çš„ç»“æœupscalerç„¶åæ”¾åˆ°å›¾ç”Ÿå›¾å†è·‘ä¸€ä¸‹å°±å¥½äº†ã€‚

```python
import torch
from torch import autocast
from diffusers import EulerAncestralDiscreteScheduler,StableDiffusionPipeline,StableDiffusionImg2ImgPipeline
from transformers import CLIPImageProcessor, CLIPProcessor, CLIPTextModel,CLIPTokenizer
from diffusers.utils import get_class_from_dynamic_module


stablepipeline = StableDiffusionPipeline.from_ckpt(
    "/data/stable-diffusion-webui/models/Stable-diffusion/majicmixRealistic_v6.safetensors",
).to("cuda")

LPWStableDiffusionPipeline = get_class_from_dynamic_module("lpw_stable_diffusion", module_file="lpw_stable_diffusion.py")
text2img = LPWStableDiffusionPipeline(**stablepipeline.components)
text2img.scheduler = EulerAncestralDiscreteScheduler.from_config(text2img.scheduler.config)
text2img.safety_checker = None

text2img.to("cuda")
prompt = "1girl"

generator = torch.Generator(device="cuda").manual_seed(1432216924)

img2img = StableDiffusionImg2ImgPipeline(**text2img.components)

with autocast("cuda"):
    images = text2img(
        prompt,
        width = 512,
        height = 512,
        num_images_per_prompt = 1,
        generator=generator,
        #negative_prompt = negative_prompt,
        num_inference_steps=20,
        guidance_scale=7).images

# è¿™é‡Œå·æ‡’äº†,åªæ˜¯æŠŠå›¾ç‰‡æ‹‰ä¼¸äº†ï¼Œå…¶å®å®Œå…¨å¯æŠ„ä¸€ä¸‹sd webuiçš„ä»£ç 
images[0] = images[0].resize((512*2, 512*2))

with autocast("cuda"):
        images = img2img(
                prompt="1girl",
                image=images[0],
                strength=0.7,
                num_inference_steps=20,
                guidance_scale=7,
                num_images_per_prompt=1,
                generator=generator
        ).images

images[0]
```

### ç›¸å¯¹äºsd webuiçš„ä¼˜ç¼ºç‚¹

ä»£ç ç»“æ„ç®€å•ã€æ¸…æ™°ã€äºŒæ¬¡å¼€å‘ç®€å•ã€æ”¯æŒ[å¤šå¡åˆ†å¸ƒå¼æ¨ç†](https://huggingface.co/docs/diffusers/v0.18.2/en/training/distributed_inference#accelerate)

[sd webuiå¤šæ˜¾å¡æ”¯æŒæƒ…å†µ](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/1621)

ä¸å¦‚sd webui åŠŸèƒ½é½å…¨ã€å¾ˆå¤šåŠŸèƒ½éœ€è¦è‡ªè¡Œç»„åˆï¼Œéå¼€ç®±å³ç”¨ æ¯”å¦‚é¢éƒ¨ä¿®å¤ã€Hires.fixç­‰

å¦‚æœè¦ç”¨diffuserså¤ç°sd webuiçš„åŠŸèƒ½è‚¯å®šæ˜¯å¯è¡Œçš„ï¼Œä¸è¿‡è¦æœ‰äº›å·¥ä½œé‡ã€‚å¥½çš„æ–¹é¢æ˜¯ç›®å‰diffusersç¤¾åŒºéå¸¸æ´»è·ƒã€æ›´æ–°é€Ÿåº¦å¾ˆå¿«ã€‚

## å†™åœ¨æœ€å

ä»¥ä¸Šç®—æ˜¯å¯¹äºstable diffusionä»¥åŠdiffusersä¸€ä¸ªçŸ­æœŸçš„å­¦ä¹ æ€»ç»“ï¼Œç”±äºæœ¬äººæ˜¯éç®—æ³•èƒŒæ™¯ï¼Œå¾ˆå¤šåœ°æ–¹è¿˜åœ¨å­¦ä¹ ä¸­ï¼Œéš¾å…æœ‰å¾ˆå¤šä¸æ­£ç¡®çš„åœ°æ–¹ï¼Œæœ‰ä»»ä½•é—®é¢˜æ¬¢è¿éšæ—¶æŒ‡å‡ºã€‚

å¦å¤–æœ¬æ–‡æœ‰å¾ˆå¤šåœ°æ–¹æ²¡æœ‰æåˆ°ï¼Œæ¯”å¦‚controlnetã€unclipã€éƒ¨ç½²ã€è®­ç»ƒç­‰ï¼Œæœ‰éœ€è¦æˆ–è€…æœ‰ç²¾åŠ›å†å¡«å‘ã€‚


## é™„1 stable diffusion webui && diffuserså®‰è£…

```bash
yum install -y git python3-opencv
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh -b

mkdir /data
mount /dev/vdb /data

conda create -n sd-webui python=3.10
conda activate sd-webui
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git
cd stable-diffusion-webui
sed -i 's/can_run_as_root=0/can_run_as_root=1/g' webui.sh && ./webui.sh 

conda create -n py3.8 python==3.8
conda activate py3.8
conda update -y conda
conda list
conda install -y numpy matplotlib pandas 
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia
conda install -c huggingface transformers diffusers
conda install -y jupyter notebook requests
```

https://pytorch.org/

https://raw.githubusercontent.com/CompVis/stable-diffusion/main/configs/stable-diffusion/v1-inference.yaml

## é™„2 å¼€å‘ç¯å¢ƒ

ä»¥ä¸Šå®ä¾‹ä»£ç éƒ½æ˜¯åœ¨jupter notebookä¸­å¯è¿è¡Œçš„ã€‚å¦‚ä»¥å…¶ä»–æ–¹å¼è¿è¡Œï¼Œéœ€è¦åšäº›å˜æ›´ã€‚

## é™„3 ä¸€äº›å‚è€ƒé“¾æ¥

- [ä¸çŸ¥é“Stable Diffusion Web UIåº”è¯¥æ€ä¹ˆé€‰ï¼Ÿ çœ‹è¿™ä¸€ç¯‡å°±å¤Ÿäº†ï¼](https://afoo.me/posts/2023-04-29-stablediffusion-webui-alternatives.html)
- https://huggingface.co/blog/controlnet
- https://pytorch.org/blog/accelerated-diffusers-pt-20/
- https://huggingface.co/docs/diffusers/v0.18.2/en/training/distributed_inference#accelerate
- [https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)